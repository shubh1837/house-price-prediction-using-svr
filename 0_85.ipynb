{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "IY1TpspXoDMH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "QbAJCdQyWRsF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, PowerTransformer, PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload kaggle.json\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!unzip house-prices-advanced-regression-techniques.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "8i_bT7RthkXM",
        "outputId": "6ea57ca0-8cd8-420a-e466-2ab6019cddc7"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cd10c6ca-000b-4231-97b7-97af416e4c4b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cd10c6ca-000b-4231-97b7-97af416e4c4b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving house-prices-advanced-regression-techniques.zip to house-prices-advanced-regression-techniques (4).zip\n",
            "Archive:  house-prices-advanced-regression-techniques.zip\n",
            "replace data_description.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: data_description.txt    \n",
            "replace sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: sample_submission.csv   \n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: test.csv                \n",
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "sample_submission = pd.read_csv('sample_submission.csv')"
      ],
      "metadata": {
        "id": "rukbPAE9hkUD"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test['SalePrice'] = sample_submission['SalePrice']\n",
        "y = train['SalePrice']\n",
        "train = train.drop('SalePrice', axis=1)"
      ],
      "metadata": {
        "id": "XcrIuDHshkRa"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = pd.concat([train, test], axis=0).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "8cp4ENpQhkO2"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Temporal Variables (Date Time Variables)\n",
        "\n",
        "for feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n",
        "\n",
        "    all_data[feature]=all_data['YrSold']-all_data[feature]"
      ],
      "metadata": {
        "id": "L7m9Cn07jo6H"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop Id column if present\n",
        "id_column = test['Id']  # Save for submission\n",
        "all_data = all_data.drop('Id', axis=1)"
      ],
      "metadata": {
        "id": "G4svF2jZhkMP"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify numerical and categorical features\n",
        "num_feats = all_data.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_feats = all_data.select_dtypes(include=['object']).columns.tolist()"
      ],
      "metadata": {
        "id": "t1IOGiC8hkJj"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Find and remove outliers in numerical features from training data (using IQR method)\n",
        "train_idx = len(train)\n",
        "train_num = all_data.iloc[:train_idx][num_feats]\n",
        "outlier_mask = np.zeros(len(train_num), dtype=bool)"
      ],
      "metadata": {
        "id": "5CWJ8x0lhkHG"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for feat in num_feats:\n",
        "    Q1 = train_num[feat].quantile(0.25)\n",
        "    Q3 = train_num[feat].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    feat_outliers = (train_num[feat] < lower) | (train_num[feat] > upper)\n",
        "    outlier_mask = outlier_mask | feat_outliers"
      ],
      "metadata": {
        "id": "g6bcyoUXhkEc"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove outliers from train\n",
        "train_clean = all_data.iloc[:train_idx][~outlier_mask]\n",
        "y_clean = y[~outlier_mask]\n",
        "print(f\"Removed {outlier_mask.sum()} outliers. New train shape: {train_clean.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVKbZNzqhkB2",
        "outputId": "63da19a9-1321-4f7f-b9e3-c3f8fe749dab"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 886 outliers. New train shape: (574, 79)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update all_data to reflect removed rows in train part\n",
        "all_data = pd.concat([train_clean, all_data.iloc[train_idx:]], axis=0).reset_index(drop=True)\n",
        "train_idx = len(train_clean)  # Update train index"
      ],
      "metadata": {
        "id": "eiUjTZG6hj-0"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Handle missing data\n",
        "# Numerical: fill with median\n",
        "for col in num_feats:\n",
        "    all_data[col] = all_data[col].fillna(all_data[col].median())\n",
        "\n",
        "# Categorical: fill with 'None'\n",
        "for col in cat_feats:\n",
        "    all_data[col] = all_data[col].fillna('None')"
      ],
      "metadata": {
        "id": "10nTF0v9hj7N"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Feature transformation for skewed features\n",
        "# Identify skewed numerical features in train\n",
        "train_num = all_data.iloc[:train_idx][num_feats]\n",
        "skewness = train_num.apply(lambda x: stats.skew(x))\n",
        "skewed_feats = skewness[abs(skewness) > 0.75].index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSMmPgqZiG3F",
        "outputId": "217b1ccc-65d7-4b85-fff4-91e670fefffc"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3262937188.py:4: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
            "  skewness = train_num.apply(lambda x: stats.skew(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Yeo-Johnson transformation\n",
        "if len(skewed_feats) > 0:\n",
        "    pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
        "    all_data[skewed_feats] = pt.fit_transform(all_data[skewed_feats])\n",
        "\n",
        "# Transform target to log (common for house prices)\n",
        "y_log = np.log(y_clean)"
      ],
      "metadata": {
        "id": "Av2mh1lCiGzs"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3.2: Feature standardization (only numerical)\n",
        "scaler = StandardScaler()\n",
        "all_data[num_feats] = scaler.fit_transform(all_data[num_feats])\n",
        "\n",
        "# One-hot encode categorical features\n",
        "all_data = pd.get_dummies(all_data)"
      ],
      "metadata": {
        "id": "mMIMk4NbiGxG"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(scaler, 'scaler.pkl')  # After fitting scaler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtijeR5r9O9c",
        "outputId": "e5903a1e-d7fd-4fde-b88a-b6ccb8b3e33e"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scaler.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split back to train and test\n",
        "X_train_full = all_data.iloc[:train_idx]\n",
        "X_test = all_data.iloc[train_idx:]"
      ],
      "metadata": {
        "id": "7Ig2CY4miGuU"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Find correlations (using train data with target)\n",
        "train_full = pd.concat([X_train_full, pd.Series(y_log, name='SalePrice')], axis=1)\n",
        "corr_matrix = train_full.corr()\n",
        "corr_sale = abs(corr_matrix['SalePrice']).sort_values(ascending=False).drop('SalePrice')"
      ],
      "metadata": {
        "id": "B-aEsQNCiGro"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Remove irrelevant features (highly correlated features > 0.8 threshold for multicollinearity)\n",
        "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "to_drop = [column for column in upper_tri.columns if any(abs(upper_tri[column]) > 0.85\n",
        "                                                         )]\n",
        "all_data = all_data.drop(to_drop, axis=1)"
      ],
      "metadata": {
        "id": "KVgAYTN_iGo2"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update train and test\n",
        "X_train_full = all_data.iloc[:train_idx]\n",
        "X_test = all_data.iloc[train_idx:]"
      ],
      "metadata": {
        "id": "h2vTvvkriGmf"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recompute correlations after drop\n",
        "train_full = pd.concat([X_train_full, pd.Series(y_log, name='SalePrice')], axis=1)\n",
        "corr_matrix = train_full.corr()\n",
        "corr_sale = abs(corr_matrix['SalePrice']).sort_values(ascending=False).drop('SalePrice')"
      ],
      "metadata": {
        "id": "rF0IOcWhiGjt"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Use only 50-60 most relevant features (based on correlation with target)\n",
        "num_features = min(60, len(corr_sale))\n",
        "top_features = corr_sale.head(num_features).index.tolist()\n",
        "joblib.dump(top_features, 'top_features.pkl')\n",
        "X_train_full = X_train_full[top_features]\n",
        "X_test = X_test[top_features]"
      ],
      "metadata": {
        "id": "xrefBx0vjCAu"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train into train and validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_log, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "v7PiRi14jEa4"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Try different regression models and print evaluation metrics\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Lasso Regression': Lasso(alpha=0.01),\n",
        "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    'SVR': SVR()\n",
        "}"
      ],
      "metadata": {
        "id": "Rl3XhpQujJWk"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Polynomial Regression separately\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_train_poly = poly.fit_transform(X_train)\n",
        "X_val_poly = poly.transform(X_val)\n",
        "model_poly = LinearRegression()\n",
        "model_poly.fit(X_train_poly, y_train)\n",
        "pred_poly = model_poly.predict(X_val_poly)\n",
        "rmse_poly = np.sqrt(mean_squared_error(y_val, pred_poly))\n",
        "r2_poly = r2_score(y_val, pred_poly)\n",
        "print(f\"Polynomial Regression: RMSE = {rmse_poly:.4f}, R2 = {r2_poly:.4f}\")\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    pred = model.predict(X_val)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
        "    r2 = r2_score(y_val, pred)\n",
        "    print(f\"{name}: RMSE = {rmse:.4f}, R2 = {r2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jF06_1njMJB",
        "outputId": "49353608-c602-4357-fc33-fc2a5c293938"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polynomial Regression: RMSE = 34.7261, R2 = -10586.1541\n",
            "Linear Regression: RMSE = 0.1414, R2 = 0.8244\n",
            "Lasso Regression: RMSE = 0.1558, R2 = 0.7868\n",
            "Decision Tree: RMSE = 0.1930, R2 = 0.6730\n",
            "Random Forest: RMSE = 0.1339, R2 = 0.8425\n",
            "SVR: RMSE = 0.1228, R2 = 0.8677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Select best model (assuming Random Forest based on common performance; adjust based on prints)\n",
        "# For demonstration, let's select Random Forest\n",
        "best_model_base = SVR()"
      ],
      "metadata": {
        "id": "mg2iikipjNz-"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "wLXDub8j8iIC"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline: scaling + SVR\n",
        "svr_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svr', SVR(kernel='rbf'))  # rbf is usually best for non-linear regression\n",
        "])\n",
        "# Parameter grid for hyperparameter tuning\n",
        "# Start with a reasonable range; you can expand based on initial results\n",
        "param_grid = {\n",
        "    'svr__C': [1, 10, 100, 1000],            # Regularization parameter\n",
        "    'svr__epsilon': [0.01, 0.1, 0.2],        # Epsilon in the epsilon-SVR model\n",
        "    'svr__gamma': ['scale', 'auto', 0.01, 0.1, 1],  # Kernel coefficient for 'rbf'\n",
        "    # You can add 'svr__kernel': ['rbf', 'linear', 'poly'] if you want to test others\n",
        "}\n",
        "\n",
        "# GridSearchCV with 5-fold cross-validation\n",
        "# Uses neg_mean_squared_error (higher is better)\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=svr_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1,          # Use all cores\n",
        "    verbose=2           # To see progress\n",
        ")\n",
        "\n",
        "# Fit on the full training data\n",
        "grid_search.fit(X_train_full, y_log)\n",
        "\n",
        "# Results\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best CV neg MSE: {grid_search.best_score_:.6f}\")\n",
        "print(f\"Best CV RMSE: {np.sqrt(-grid_search.best_score_):.6f}\")\n",
        "\n",
        "# Best tuned model\n",
        "best_svr_model = grid_search.best_estimator_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Q43qClZjRG2",
        "outputId": "253a631d-a656-41c1-9fed-b94d7df75ddf"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
            "Best parameters: {'svr__C': 1, 'svr__epsilon': 0.01, 'svr__gamma': 0.01}\n",
            "Best CV neg MSE: -0.023823\n",
            "Best CV RMSE: 0.154348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Model packing - save to pkl file\n",
        "joblib.dump(best_model, 'best_house_price_model.pkl')\n",
        "print(\"Model saved as 'best_house_price_model.pkl'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxAkeNBqjRJ_",
        "outputId": "366b37db-5c3c-450c-8486-769dfb43c2ab"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as 'best_house_price_model.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Generate predictions for test and save submission\n",
        "pred_test_log = best_model.predict(X_test)\n",
        "pred_test = np.exp(pred_test_log)\n",
        "submission = pd.DataFrame({'Id': id_column, 'SalePrice': pred_test})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission file created as 'submission.csv'\")"
      ],
      "metadata": {
        "id": "G6SpINxZjRMm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2fdd4d5-55bb-4433-9318-fa7aef2ea846"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission file created as 'submission.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7INnKO_hjRPz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}